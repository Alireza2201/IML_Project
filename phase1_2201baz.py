# -*- coding: utf-8 -*-
"""phase1-2201baz

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h6qwgEUFsxtmQ2ePYLNKsii2VfhyHCSF

> SHARIF UNI


> IML project phase 1 - part 2 and 3


> ðŸ”“ Alireza Haghshenas & Abolfazl Gargoori

# Part 2 - Private Training

***In this section, you will first train a classification model using a standard approach, then train it with privacy enhancements, and compare the MIA accuracy of both models. (Use the provided model in model.py for all tasks.)***

### Simulation Question 4.

**Use 80 percent of the CIFAR-10 training data to train your model. This will serve as your baseline model.**

Importing Libraries and Defining the Model :
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, random_split
import matplotlib.pyplot as plt

class CIFAR10Classifier(nn.Module):
    def __init__(self):
        super(CIFAR10Classifier, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, 3, 1)
        self.conv2 = nn.Conv2d(16, 32, 3, 1)
        self.dropout1 = nn.Dropout2d(0.25)
        self.dropout2 = nn.Dropout2d(0.5)
        self.fc1 = nn.Linear(6272, 64)
        self.fc2 = nn.Linear(64, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        return x

"""Data Transformations and Loading :"""

# Transformations for CIFAR-10 dataset
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
])

# Load CIFAR-10 dataset
train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

#  80% training and 20% validation
train_size = int(0.8 * len(train_dataset))
val_size = len(train_dataset) - train_size
train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])

# Data loaders
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

"""Initializing the Model, Loss Function, and Optimizer :"""

model = CIFAR10Classifier()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

"""Training the Model :"""

num_epochs = 10
train_loss_history = []
val_accuracy_history = []

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    epoch_loss = running_loss / len(train_loader)
    train_loss_history.append(epoch_loss)

    # Calculate validation accuracy
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    val_accuracy = 100 * correct / total
    val_accuracy_history.append(val_accuracy)
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')

"""Evaluating and Saving the Model"""

# Evaluate the model on the test dataset
model.eval()
correct = 0
total = 0

with torch.no_grad():
    for inputs, labels in test_loader:
        outputs = model(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

test_accuracy = 100 * correct / total
print(f'Test Accuracy: {test_accuracy:.2f}%')

# Save the trained model
torch.save(model.state_dict(), 'cifar10_baseline_model.pth')

# Save the training and validation results
results_dict = {
    'train_loss_history': train_loss_history,
    'val_accuracy_history': val_accuracy_history,
    'test_accuracy': test_accuracy
}
torch.save(results_dict, 'training_results.dict')

print("Training complete.")

"""plot the result 2201baz"""

plt.figure(figsize=(5, 6))

plt.subplot(2, 1, 1)
plt.plot(range(1, num_epochs + 1), train_loss_history, marker='o', color='b', label='Train Loss')
plt.title('Training Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.subplot(2, 1, 2)
plt.plot(range(1, num_epochs + 1), val_accuracy_history, marker='o', color='g', label='Validation Accuracy')
plt.title('Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""### Simulation Question 5.

**Train your baseline model with privacy enhancements. This is your modified model. Ensure that the test accuracy difference between your baseline model and the modified model is less than 15**
"""

pip install opacus

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, random_split
from opacus import PrivacyEngine
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Define the CIFAR-10 Classifier model
class CIFAR10Classifier(nn.Module):
    def __init__(self):
        super(CIFAR10Classifier, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, 3, 1)
        self.conv2 = nn.Conv2d(16, 32, 3, 1)
        self.dropout1 = nn.Dropout2d(0.25)
        self.dropout2 = nn.Dropout2d(0.5)
        self.fc1 = nn.Linear(6272, 64)
        self.fc2 = nn.Linear(64, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        return x

"""Data Preparation"""

# Transformations for CIFAR-10 dataset
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
])

# Load CIFAR-10 dataset
train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

#  80% training and 20% validation
train_size = int(0.8 * len(train_dataset))
val_size = len(train_dataset) - train_size
train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])

# Data loaders
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

"""Load Baseline Model"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
baseline_model = CIFAR10Classifier().to(device)
baseline_model.load_state_dict(torch.load('cifar10_baseline_model.pth'))
baseline_model.eval()

"""Train Model with Privacy Enhancements"""

def train_model_with_privacy(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=25):
    privacy_engine = PrivacyEngine()
    model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(
        module=model,
        optimizer=optimizer,
        data_loader=train_loader,
        target_epsilon=6, # Adjust epsilon as needed for privacy budget
        target_delta=1e-5,
        epochs=num_epochs,
        max_grad_norm=1.0,
    )

    train_loss_history = []
    val_accuracy_history = []

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        epoch_loss = running_loss / len(train_loader)
        train_loss_history.append(epoch_loss)

        # Calculate validation accuracy
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        val_accuracy = 100 * correct / total
        val_accuracy_history.append(val_accuracy)
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')

    return model, train_loss_history, val_accuracy_history

# Initialize and train the model with privacy enhancements
privacy_model = CIFAR10Classifier().to(device)
criterion = nn.CrossEntropyLoss()
privacy_optimizer = optim.Adam(privacy_model.parameters(), lr=0.0005)
privacy_model, privacy_train_loss_history, privacy_val_accuracy_history = train_model_with_privacy(
    privacy_model, train_loader, val_loader, criterion, privacy_optimizer, device, num_epochs=25)

"""Evaluate Models and Compare Results"""

def evaluate_model(model, dataloader):
    model.eval()
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    accuracy = accuracy_score(all_labels, all_preds)
    return accuracy

baseline_test_accuracy = evaluate_model(baseline_model, test_loader)
print(f'Baseline Test Accuracy: {baseline_test_accuracy*100:.2f}%')

privacy_test_accuracy = evaluate_model(privacy_model, test_loader)
print(f'Privacy Test Accuracy: {privacy_test_accuracy*100:.2f}%')

accuracy_difference = baseline_test_accuracy - privacy_test_accuracy
print(f'Accuracy Difference: {accuracy_difference*100:.2f}%')
assert accuracy_difference < 15, "Accuracy difference is more than 15%"

"""Plotting the Results"""

plt.figure(figsize=(6, 7))

plt.subplot(2, 1, 1)
plt.plot(range(1, len(privacy_train_loss_history) + 1), privacy_train_loss_history, marker='o', color='b', label='Train Loss')
plt.title('Training Loss with Privacy Enhancements')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.subplot(2, 1, 2)
plt.plot(range(1, len(privacy_val_accuracy_history) + 1), privacy_val_accuracy_history, marker='o', color='g', label='Validation Accuracy')
plt.title('Validation Accuracy with Privacy Enhancements')
plt.xlabel('Epochs')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.grid(True)


plt.tight_layout()
plt.show()

"""Save the Modified Model and Results"""

# Save the privacy-enhanced model correctly
def save_model_correctly(model, path):
    # Remove the "_module" prefix
    state_dict = model.state_dict()
    new_state_dict = {k.replace("_module.", ""): v for k, v in state_dict.items()}
    torch.save(new_state_dict, path)

save_model_correctly(privacy_model, 'cifar10_privacy_model.pth')

# Save the training and validation results for the privacy-enhanced model
privacy_results_dict = {
    'train_loss_history': privacy_train_loss_history,
    'val_accuracy_history': privacy_val_accuracy_history,
    'test_accuracy': privacy_test_accuracy
}
torch.save(privacy_results_dict, 'privacy_training_results.dict')

print("Training with privacy enhancements complete.")

"""### Simulation Question 6.

**Train two Attacker Models based on MIA techniques learned in Phase 0, one for the baseline model and one for the modified model. Compare the MIA accuracy of these two attacker models. Use 80 percent of the training data as your seen data, and the remaining training data along with the test data as your unseen data.**

Import Libraries and Define Models
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, random_split, Subset
from sklearn.metrics import accuracy_score
import numpy as np
import matplotlib.pyplot as plt

# Define the CIFAR-10 Classifier model
class CIFAR10Classifier(nn.Module):
    def __init__(self):
        super(CIFAR10Classifier, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, 3, 1)
        self.conv2 = nn.Conv2d(16, 32, 3, 1)
        self.dropout1 = nn.Dropout2d(0.25)
        self.dropout2 = nn.Dropout2d(0.5)
        self.fc1 = nn.Linear(6272, 64)
        self.fc2 = nn.Linear(64, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        return x

# Define the MIA Attacker model
class AttackerModel(nn.Module):
    def __init__(self):
        super(AttackerModel, self).__init__()
        self.fc1 = nn.Linear(10, 64)
        self.fc2 = nn.Linear(64, 2)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

"""Load Data and Initialize Models"""

# Transformations for CIFAR-10 dataset
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
])

# Load CIFAR-10 dataset
train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

#80% seen and 20% unseen
train_size = int(0.8 * len(train_dataset))
seen_data, unseen_data = random_split(train_dataset, [train_size, len(train_dataset) - train_size])

# Combine the unseen training data and test data for the attacker model's unseen data
unseen_data = Subset(train_dataset, unseen_data.indices + list(range(len(test_dataset))))

# Data loaders
BATCH_SIZE = 64
seen_loader = DataLoader(seen_data, batch_size=BATCH_SIZE, shuffle=True)
unseen_loader = DataLoader(unseen_data, batch_size=BATCH_SIZE, shuffle=False)

"""Initialize and Load Pre-trained Models"""

# Initialize models
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
baseline_model = CIFAR10Classifier().to(device)
privacy_model = CIFAR10Classifier().to(device)

baseline_model.load_state_dict(torch.load('cifar10_baseline_model.pth'))
privacy_model.load_state_dict(torch.load('cifar10_privacy_model.pth'))

baseline_model.eval()
privacy_model.eval()

"""Generate Query Data Using Shadow Models"""

def generate_query_data_shadow_models(model_class, data_loader, label, model_path, num_shadow_models=5):
    features = []
    labels = []
    for _ in range(num_shadow_models):
        shadow_model = model_class().to(device)
        shadow_model.load_state_dict(torch.load(model_path))  # Load the specified model
        shadow_model.eval()
        with torch.no_grad():
            for inputs, _ in data_loader:
                inputs = inputs.to(device)
                outputs = shadow_model(inputs)
                features.extend(outputs.cpu().numpy())
                labels.extend([label] * outputs.size(0))
    return features, labels

# Generate query data for attacker models using multiple shadow models
baseline_seen_features, baseline_seen_labels = generate_query_data_shadow_models(CIFAR10Classifier, seen_loader, 1, 'cifar10_baseline_model.pth', num_shadow_models=10)
baseline_unseen_features, baseline_unseen_labels = generate_query_data_shadow_models(CIFAR10Classifier, unseen_loader, 0, 'cifar10_baseline_model.pth', num_shadow_models=10)
privacy_seen_features, privacy_seen_labels = generate_query_data_shadow_models(CIFAR10Classifier, seen_loader, 1, 'cifar10_privacy_model.pth', num_shadow_models=10)
privacy_unseen_features, privacy_unseen_labels = generate_query_data_shadow_models(CIFAR10Classifier, unseen_loader, 0, 'cifar10_privacy_model.pth', num_shadow_models=10)

"""Prepare Data for Attacker Models"""

# Combine features and labels
baseline_features = baseline_seen_features + baseline_unseen_features
baseline_labels = baseline_seen_labels + baseline_unseen_labels
privacy_features = privacy_seen_features + privacy_unseen_features
privacy_labels = privacy_seen_labels + privacy_unseen_labels

# Convert to tensors
baseline_features = torch.tensor(baseline_features, dtype=torch.float32)
baseline_labels = torch.tensor(baseline_labels, dtype=torch.long)
privacy_features = torch.tensor(privacy_features, dtype=torch.float32)
privacy_labels = torch.tensor(privacy_labels, dtype=torch.long)

# Data loaders for attacker models
baseline_loader = DataLoader(list(zip(baseline_features, baseline_labels)), batch_size=64, shuffle=True)
privacy_loader = DataLoader(list(zip(privacy_features, privacy_labels)), batch_size=64, shuffle=True)

"""Initialize and Train Attacker Models"""

baseline_attacker = AttackerModel().to(device)
privacy_attacker = AttackerModel().to(device)

# Loss and optimizer for attacker models
criterion = nn.CrossEntropyLoss()
optimizer_baseline = optim.Adam(baseline_attacker.parameters(), lr=0.001)
optimizer_privacy = optim.Adam(privacy_attacker.parameters(), lr=0.001)

# Function to train attacker model and print accuracy in each epoch
def train_attacker(attacker_model, optimizer, data_loader, num_epochs=10):
    loss_history = []
    accuracy_history = []
    for epoch in range(num_epochs):
        attacker_model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        for inputs, labels in data_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = attacker_model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        epoch_loss = running_loss / len(data_loader)
        epoch_accuracy = correct / total
        loss_history.append(epoch_loss)
        accuracy_history.append(epoch_accuracy)
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.5f}, Accuracy: {epoch_accuracy:.8f}')
    return loss_history, accuracy_history

print("Training baseline attacker model...")
baseline_loss_history, baseline_accuracy_history = train_attacker(baseline_attacker, optimizer_baseline, baseline_loader)

print("Training privacy attacker model...")
privacy_loss_history, privacy_accuracy_history = train_attacker(privacy_attacker, optimizer_privacy, privacy_loader)

"""Evaluate Attacker Models and Plot Results"""

def evaluate_attacker(attacker_model, features, labels):
    attacker_model.eval()
    with torch.no_grad():
        outputs = attacker_model(features.to(device))
        _, predicted = torch.max(outputs, 1)
        accuracy = accuracy_score(labels.cpu(), predicted.cpu())
    return accuracy

baseline_attacker_accuracy = evaluate_attacker(baseline_attacker, baseline_features, baseline_labels)
privacy_attacker_accuracy = evaluate_attacker(privacy_attacker, privacy_features, privacy_labels)

print(f'Baseline Attacker Model Accuracy: {baseline_attacker_accuracy:.5f}')
print(f'Privacy Attacker Model Accuracy: {privacy_attacker_accuracy:.5f}')

plt.figure(figsize=(7, 8))
plt.title('Loss and Accuracy History of Attacker Models')
plt.subplot(2, 1, 1)
plt.plot(range(1, 11), baseline_loss_history, label='Baseline Attacker Model - Loss')
plt.plot(range(1, 11), privacy_loss_history, label='Privacy Attacker Model - Loss')
plt.legend()
plt.grid(True)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.subplot(2, 1, 2)
plt.plot(range(1, 11), baseline_accuracy_history, label='Baseline Attacker Model - Accuracy')
plt.plot(range(1, 11), privacy_accuracy_history, label='Privacy Attacker Model - Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')

plt.legend()
plt.grid(True)
plt.show()

"""### Simulation Question 7.

**Improve your attacker models to achieve better MIA accuracy for both the baseline and modified models (e.g., by increasing the number of shadow models) Then compare the new accuracies with the previous results.**

Import Libraries and Define Models
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, random_split, Subset
from sklearn.metrics import accuracy_score
import numpy as np
import matplotlib.pyplot as plt

# Define the CIFAR-10 Classifier model
class CIFAR10Classifier(nn.Module):
    def __init__(self):
        super(CIFAR10Classifier, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, 3, 1)
        self.conv2 = nn.Conv2d(16, 32, 3, 1)
        self.dropout1 = nn.Dropout2d(0.25)
        self.dropout2 = nn.Dropout2d(0.5)
        self.fc1 = nn.Linear(6272, 64)
        self.fc2 = nn.Linear(64, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        return x

# Define the MIA Attacker model
class AttackerModel(nn.Module):
    def __init__(self):
        super(AttackerModel, self).__init__()
        self.fc1 = nn.Linear(10, 64)
        self.fc2 = nn.Linear(64, 2)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

"""Load Data and Initialize Models"""

# Transformations for CIFAR-10 dataset
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
])

# Load CIFAR-10 dataset
train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

# Split the training data into 80% seen and 20% unseen
train_size = int(0.8 * len(train_dataset))
seen_data, unseen_data = random_split(train_dataset, [train_size, len(train_dataset) - train_size])

# Combine the unseen training data and test data for the attacker model's unseen data
unseen_data = Subset(train_dataset, unseen_data.indices + list(range(len(test_dataset))))

# Data loaders
BATCH_SIZE = 64
seen_loader = DataLoader(seen_data, batch_size=BATCH_SIZE, shuffle=True)
unseen_loader = DataLoader(unseen_data, batch_size=BATCH_SIZE, shuffle=False)

"""Generate Query Data from Enhanced Shadow Models"""

# Initialize models
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
baseline_model = CIFAR10Classifier().to(device)
privacy_model = CIFAR10Classifier().to(device)

# Load pre-trained models
baseline_model.load_state_dict(torch.load('cifar10_baseline_model.pth'))
privacy_model.load_state_dict(torch.load('cifar10_privacy_model.pth'))

baseline_model.eval()
privacy_model.eval()

"""Train and Evaluate Attacker Models"""

# Function to generate query data using shadow models
def generate_query_data_shadow_models(model_class, data_loader, label, model_path, num_shadow_models=20):
    features = []
    labels = []
    for _ in range(num_shadow_models):
        shadow_model = model_class().to(device)
        shadow_model.load_state_dict(torch.load(model_path))  # Load the specified model
        shadow_model.eval()
        with torch.no_grad():
            for inputs, _ in data_loader:
                inputs = inputs.to(device)
                outputs = shadow_model(inputs)
                features.extend(outputs.cpu().numpy())
                labels.extend([label] * outputs.size(0))
    return features, labels

# Generate query data for attacker models using multiple shadow models
baseline_seen_features, baseline_seen_labels = generate_query_data_shadow_models(CIFAR10Classifier, seen_loader, 1, 'cifar10_baseline_model.pth', num_shadow_models=20)
baseline_unseen_features, baseline_unseen_labels = generate_query_data_shadow_models(CIFAR10Classifier, unseen_loader, 0, 'cifar10_baseline_model.pth', num_shadow_models=20)
privacy_seen_features, privacy_seen_labels = generate_query_data_shadow_models(CIFAR10Classifier, seen_loader, 1, 'cifar10_privacy_model.pth', num_shadow_models=20)
privacy_unseen_features, privacy_unseen_labels = generate_query_data_shadow_models(CIFAR10Classifier, unseen_loader, 0, 'cifar10_privacy_model.pth', num_shadow_models=20)

# # Function to generate query data using shadow models
# def generate_query_data_shadow_models(model_class, data_loader, label, model_path, num_shadow_models=100):
#     features = []
#     labels = []
#     for _ in range(num_shadow_models):
#         shadow_model = model_class().to(device)
#         shadow_model.load_state_dict(torch.load(model_path))  # Load the specified model
#         shadow_model.eval()
#         with torch.no_grad():
#             for inputs, _ in data_loader:
#                 inputs = inputs.to(device)
#                 outputs = shadow_model(inputs)
#                 features.extend(outputs.cpu().numpy())
#                 labels.extend([label] * outputs.size(0))
#     return features, labels

# # Generate query data for attacker models using multiple shadow models
# baseline_seen_features, baseline_seen_labels = generate_query_data_shadow_models(CIFAR10Classifier, seen_loader, 1, 'cifar10_baseline_model.pth', num_shadow_models=100)
# baseline_unseen_features, baseline_unseen_labels = generate_query_data_shadow_models(CIFAR10Classifier, unseen_loader, 0, 'cifar10_baseline_model.pth', num_shadow_models=100)
# privacy_seen_features, privacy_seen_labels = generate_query_data_shadow_models(CIFAR10Classifier, seen_loader, 1, 'cifar10_privacy_model.pth', num_shadow_models=100)
# privacy_unseen_features, privacy_unseen_labels = generate_query_data_shadow_models(CIFAR10Classifier, unseen_loader, 0, 'cifar10_privacy_model.pth', num_shadow_models=100)

# Combine features and labels
baseline_features = baseline_seen_features + baseline_unseen_features
baseline_labels = baseline_seen_labels + baseline_unseen_labels
privacy_features = privacy_seen_features + privacy_unseen_features
privacy_labels = privacy_seen_labels + privacy_unseen_labels

# Convert to tensors
baseline_features = torch.tensor(baseline_features, dtype=torch.float32)
baseline_labels = torch.tensor(baseline_labels, dtype=torch.long)
privacy_features = torch.tensor(privacy_features, dtype=torch.float32)
privacy_labels = torch.tensor(privacy_labels, dtype=torch.long)

# Data loaders for attacker models
baseline_loader = DataLoader(list(zip(baseline_features, baseline_labels)), batch_size=64, shuffle=True)
privacy_loader = DataLoader(list(zip(privacy_features, privacy_labels)), batch_size=64, shuffle=True)

# Initialize attacker models
baseline_attacker = AttackerModel().to(device)
privacy_attacker = AttackerModel().to(device)

# Loss and optimizer for attacker models
criterion = nn.CrossEntropyLoss()
optimizer_baseline = optim.Adam(baseline_attacker.parameters(), lr=0.001)
optimizer_privacy = optim.Adam(privacy_attacker.parameters(), lr=0.001)

# Function to train attacker model and print accuracy in each epoch
def train_attacker(attacker_model, optimizer, data_loader, num_epochs=10):
    loss_history = []
    accuracy_history = []
    for epoch in range(num_epochs):
        attacker_model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        for inputs, labels in data_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = attacker_model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        epoch_loss = running_loss / len(data_loader)
        epoch_accuracy = correct / total
        loss_history.append(epoch_loss)
        accuracy_history.append(epoch_accuracy)
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.5f}, Accuracy: {epoch_accuracy:.8f}')
    return loss_history, accuracy_history

# Train attacker models
print("Training baseline attacker model...")
baseline_loss_history, baseline_accuracy_history = train_attacker(baseline_attacker, optimizer_baseline, baseline_loader)

print("Training privacy attacker model...")
privacy_loss_history, privacy_accuracy_history = train_attacker(privacy_attacker, optimizer_privacy, privacy_loader)

def evaluate_attacker(attacker_model, features, labels):
    attacker_model.eval()
    with torch.no_grad():
        outputs = attacker_model(features.to(device))
        _, predicted = torch.max(outputs, 1)
        accuracy = accuracy_score(labels.cpu(), predicted.cpu())
    return accuracy

baseline_attacker_accuracy = evaluate_attacker(baseline_attacker, baseline_features, baseline_labels)
privacy_attacker_accuracy = evaluate_attacker(privacy_attacker, privacy_features, privacy_labels)

print(f'Baseline Attacker Model Accuracy: {baseline_attacker_accuracy:.5f}')
print(f'Privacy Attacker Model Accuracy: {privacy_attacker_accuracy:.5f}')

plt.figure(figsize=(7, 8))
plt.title('Loss and Accuracy History of Attacker Models')
plt.subplot(2, 1, 1)
plt.plot(range(1, 11), baseline_loss_history, label='Baseline Attacker Model - Loss')
plt.plot(range(1, 11), privacy_loss_history, label='Privacy Attacker Model - Loss')
plt.legend()
plt.grid(True)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.subplot(2, 1, 2)
plt.plot(range(1, 11), baseline_accuracy_history, label='Baseline Attacker Model - Accuracy')
plt.plot(range(1, 11), privacy_accuracy_history, label='Privacy Attacker Model - Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')

plt.legend()
plt.grid(True)
plt.show()

"""# part 3 - Membership Inference Attack

### Simulation Question 8.

**Attempt to train an attacker model for the given private model (private_model.pth). We will test it on our dataset during the online presentation session. A competitive bonus point is available for the best performance.**

Define the CIFAR-10 Classifier Model
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class CIFAR10Classifier(nn.Module):
    def __init__(self):
        super(CIFAR10Classifier, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, 3, 1)
        self.conv2 = nn.Conv2d(16, 32, 3, 1)
        self.dropout1 = nn.Dropout2d(0.25)
        self.dropout2 = nn.Dropout2d(0.5)
        self.fc1 = nn.Linear(6272, 64)
        self.fc2 = nn.Linear(64, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        return x

"""Load and Prepare the Data"""

import torch
import torch.optim as optim
from torchvision.datasets import CIFAR10
from torchvision import transforms
from torch.utils.data import Subset, DataLoader, TensorDataset

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load CIFAR-10 dataset
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

full_train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)

# Load the indices from list.txt
indices_file = 'list.txt'
with open(indices_file, 'r') as f:
    indices = [int(line.strip()) for line in f]

train_indices_set = set(indices)
all_indices = set(range(len(full_train_dataset)))
other_indices = list(all_indices - train_indices_set)

train_dataset = Subset(full_train_dataset, indices[:len(indices)//2])
other_dataset = Subset(full_train_dataset, other_indices)

# Create data loaders
BATCH_SIZE = 64
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)
other_loader = DataLoader(other_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

# Create labels
train_labels = torch.ones(len(train_dataset)).to(device)
other_labels = torch.zeros(len(other_dataset)).to(device)
test_labels = torch.zeros(len(test_dataset)).to(device)

"""Load Pre-Trained Model and Extract Features"""

def extract_features(model, dataloader):
    model.eval()
    features = []
    with torch.no_grad():
        for data in dataloader:
            inputs, _ = data
            inputs = inputs.to(device)
            outputs = model(inputs)
            features.append(outputs)
    return torch.cat(features).to(device)

model = CIFAR10Classifier()
state_dict = torch.load("private_model.pth", map_location=device)
new_state_dict = {key.replace('_module.', ''): value for key, value in state_dict.items()}
model.load_state_dict(new_state_dict)
model.to(device)
model.eval()

train_features = extract_features(model, train_loader)
other_features = extract_features(model, other_loader)
test_features = extract_features(model, test_loader)

combined_features = torch.cat((train_features, other_features, test_features))
combined_labels = torch.cat((train_labels, other_labels, test_labels))

new_dataset = TensorDataset(combined_features, combined_labels)
new_loader = DataLoader(new_dataset, batch_size=BATCH_SIZE, shuffle=True)

"""Define and Train the Attacker Model"""

class AttackerModel(nn.Module):
    def __init__(self):
        super(AttackerModel, self).__init__()
        self.fc1 = nn.Linear(10, 64)
        self.fc2 = nn.Linear(64, 2)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

attacker = AttackerModel().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(attacker.parameters(), lr=0.001)

def train_attacker(attacker_model, optimizer, data_loader, num_epochs=10):
    loss_history = []
    for epoch in range(num_epochs):
        attacker_model.train()
        running_loss = 0.0
        for inputs, labels in data_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = attacker_model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
        epoch_loss = running_loss / len(data_loader)
        loss_history.append(epoch_loss)
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')
    return loss_history

loss_history = train_attacker(attacker, optimizer, new_loader)

"""Evaluate the Attacker Model"""

from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score

def evaluate_attacker(attacker_model, data_loader):
    attacker_model.eval()
    all_labels = []
    all_predicted = []
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in data_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = attacker_model(inputs)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            all_labels.extend(labels.cpu().numpy())
            all_predicted.extend(predicted.cpu().numpy())
    accuracy = correct / total
    cm = confusion_matrix(all_labels, all_predicted)
    precision = precision_score(all_labels, all_predicted)
    recall = recall_score(all_labels, all_predicted)
    f1 = f1_score(all_labels, all_predicted)
    return accuracy, cm, precision, recall, f1

attacker_accuracy, cm, precision, recall, f1 = evaluate_attacker(attacker, new_loader)
print(f'Attacker Model Accuracy: {attacker_accuracy:.4f}')
print(f'Confusion Matrix:\n{cm}')
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

plt.figure(figsize=(12, 6))
plt.plot(range(1, 11), loss_history, label='Attacker Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss History of Attacker Model')
plt.legend()
plt.grid(True)
plt.show()