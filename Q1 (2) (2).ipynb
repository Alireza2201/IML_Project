{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30733,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler, random_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "class CustomResNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CustomResNet, self).__init__()\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "        self.features = nn.Sequential(*list(resnet.children())[:-1])  # Remove the last fully connected layer\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(resnet.fc.in_features, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    best_model_wts = model.state_dict()\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = model.state_dict()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(nn.Softmax(dim=1)(outputs).cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    auroc = roc_auc_score(np.eye(10)[all_labels], np.array(all_probs), average='weighted', multi_class='ovr')\n",
        "\n",
        "    return accuracy, precision, recall, f1, auroc\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "indices = np.arange(len(train_dataset))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "S_values = [5, 10, 20]\n",
        "R_values = [5, 10, 20]\n",
        "\n",
        "results = {}\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "trained_modelss = {}\n",
        "for S in S_values:\n",
        "    for R in R_values:\n",
        "        print(f'Training with S={S} shards and R={R} repetitions')\n",
        "        shard_size = len(train_dataset) // S\n",
        "        shard_indices = [indices[i:i+shard_size] for i in range(0, len(train_dataset), shard_size)]\n",
        "\n",
        "        trained_models = []\n",
        "        for k in range(S):\n",
        "            shard_data = [train_dataset[i] for i in shard_indices[k]]\n",
        "            shard_length = len(shard_data)\n",
        "            val_length = shard_length // 5\n",
        "            train_length = shard_length - val_length\n",
        "            train_data, val_data = random_split(shard_data, [train_length, val_length])\n",
        "            train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "            val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
        "            dataloaders = {'train': train_loader, 'val': val_loader}\n",
        "            model = CustomResNet(num_classes=10)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "            scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "            trained_model = train_model(model, dataloaders=dataloaders, criterion=criterion, optimizer=optimizer, scheduler=scheduler, num_epochs=11)\n",
        "            trained_models.append(trained_model)\n",
        "\n",
        "        trained_modelss[f'S={S}, R={R}'] = trained_models\n",
        "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "        accuracy, precision, recall, f1, auroc = evaluate_model(model, test_loader)\n",
        "        results[f'S={S}, R={R}'] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-score': f1, 'AUROC': auroc }\n",
        "\n",
        "for key, value in results.items():\n",
        "    print(f'{key}: {value}')\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ntkl7o8XFq3Q",
        "outputId": "362306b3-46b9-415e-ae5b-4b6f69938a44",
        "execution": {
          "iopub.status.busy": "2024-07-02T20:26:51.413993Z",
          "iopub.execute_input": "2024-07-02T20:26:51.414489Z",
          "iopub.status.idle": "2024-07-02T20:55:59.636121Z",
          "shell.execute_reply.started": "2024-07-02T20:26:51.414439Z",
          "shell.execute_reply": "2024-07-02T20:55:59.635169Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Files already downloaded and verified\nFiles already downloaded and verified\nTraining with S=5 shards and R=5 repetitions\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training with S=5 shards and R=10 repetitions\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training with S=5 shards and R=20 repetitions\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training with S=10 shards and R=5 repetitions\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training with S=10 shards and R=10 repetitions\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training with S=10 shards and R=20 repetitions\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training with S=20 shards and R=5 repetitions\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training with S=20 shards and R=10 repetitions\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training with S=20 shards and R=20 repetitions\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "S=5, R=5: {'Accuracy': 0.7609999999999999, 'Precision': 0.7614155193893641, 'Recall': 0.7609999999999999, 'F1-score': 0.7573236489456205, 'AUROC': 0.9456405055555555}\nS=5, R=10: {'Accuracy': 0.7767999999999999, 'Precision': 0.7779544969850951, 'Recall': 0.7767999999999999, 'F1-score': 0.7757606601376628, 'AUROC': 0.947765861111111}\nS=5, R=20: {'Accuracy': 0.7607999999999999, 'Precision': 0.7695257940373972, 'Recall': 0.7607999999999999, 'F1-score': 0.7633722185616232, 'AUROC': 0.9444403611111111}\nS=10, R=5: {'Accuracy': 0.7244999999999999, 'Precision': 0.7294886234703635, 'Recall': 0.7244999999999999, 'F1-score': 0.7246055298050293, 'AUROC': 0.9264217611111111}\nS=10, R=10: {'Accuracy': 0.7179, 'Precision': 0.7224119496713524, 'Recall': 0.7179, 'F1-score': 0.7170915538116818, 'AUROC': 0.9307582111111111}\nS=10, R=20: {'Accuracy': 0.7203999999999999, 'Precision': 0.7200936945977561, 'Recall': 0.7203999999999999, 'F1-score': 0.7182527588017926, 'AUROC': 0.9284757388888889}\nS=20, R=5: {'Accuracy': 0.6358999999999999, 'Precision': 0.6421541733377121, 'Recall': 0.6358999999999999, 'F1-score': 0.6295461685634913, 'AUROC': 0.8944996611111111}\nS=20, R=10: {'Accuracy': 0.6374, 'Precision': 0.6422450973202622, 'Recall': 0.6374, 'F1-score': 0.6367318282666525, 'AUROC': 0.8919029444444444}\nS=20, R=20: {'Accuracy': 0.6387, 'Precision': 0.6573699977056655, 'Recall': 0.6387, 'F1-score': 0.6418221295955533, 'AUROC': 0.8962067777777777}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "forget_indices = np.random.choice(len(train_dataset), 500, replace=False)\n",
        "forget_set = [train_dataset[i] for i in forget_indices]\n",
        "forget_labels = [train_dataset[i][1] for i in forget_indices]\n",
        "\n",
        "unlearned_models_dict = {}\n",
        "\n",
        "for key, shard_models in trained_modelss.items():\n",
        "    S = int(key.split(\",\")[0][2:])\n",
        "    R = int(key.split(\",\")[1][3:])\n",
        "    shard_size = len(train_dataset) // S\n",
        "    shard_indices = [indices[i:i+shard_size] for i in range(0, len(train_dataset), shard_size)]\n",
        "\n",
        "    for k in range(S):\n",
        "        shard_data_indices = set(shard_indices[k])\n",
        "        forget_data_indices_in_shard = [idx for idx in forget_indices if idx in shard_data_indices]\n",
        "\n",
        "        if not forget_data_indices_in_shard:\n",
        "            continue\n",
        "\n",
        "        updated_shard_data = [train_dataset[i] for i in shard_indices[k] if i not in forget_data_indices_in_shard]\n",
        "        shard_length = len(updated_shard_data)\n",
        "        val_length = shard_length // 5\n",
        "        train_length = shard_length - val_length\n",
        "        train_data, val_data = random_split(updated_shard_data, [train_length, val_length])\n",
        "        train_loader = DataLoader(train_data, batch_size=64, shuffle=True, drop_last=True)\n",
        "        val_loader = DataLoader(val_data, batch_size=64, shuffle=False, drop_last=True)\n",
        "        dataloaders = {'train': train_loader, 'val': val_loader}\n",
        "        model = CustomResNet(num_classes=10)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "        scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "        trained_model = train_model(model, dataloaders=dataloaders, criterion=criterion, optimizer=optimizer, scheduler=scheduler, num_epochs=15)\n",
        "        shard_models[k] = trained_model\n",
        "\n",
        "    unlearned_models_dict[key] = shard_models\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, drop_last=True)\n",
        "    accuracy, precision, recall, f1, auroc = evaluate_model(model, test_loader)\n",
        "    results[f'{key}, Unlearned'] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-score': f1, 'AUROC': auroc}\n",
        "\n",
        "for key, value in results.items():\n",
        "    print(f'{key}: {value}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-hXDGqrFLt2",
        "outputId": "9badd927-0b7f-432e-87c6-d427bb773f39",
        "execution": {
          "iopub.status.busy": "2024-07-02T22:05:12.090416Z",
          "iopub.execute_input": "2024-07-02T22:05:12.090876Z",
          "iopub.status.idle": "2024-07-02T22:41:18.048153Z",
          "shell.execute_reply.started": "2024-07-02T22:05:12.090847Z",
          "shell.execute_reply": "2024-07-02T22:41:18.047265Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "S=5, R=5: {'Accuracy': 0.7609999999999999, 'Precision': 0.7614155193893641, 'Recall': 0.7609999999999999, 'F1-score': 0.7573236489456205, 'AUROC': 0.9456405055555555}\nS=5, R=10: {'Accuracy': 0.7767999999999999, 'Precision': 0.7779544969850951, 'Recall': 0.7767999999999999, 'F1-score': 0.7757606601376628, 'AUROC': 0.947765861111111}\nS=5, R=20: {'Accuracy': 0.7607999999999999, 'Precision': 0.7695257940373972, 'Recall': 0.7607999999999999, 'F1-score': 0.7633722185616232, 'AUROC': 0.9444403611111111}\nS=10, R=5: {'Accuracy': 0.7244999999999999, 'Precision': 0.7294886234703635, 'Recall': 0.7244999999999999, 'F1-score': 0.7246055298050293, 'AUROC': 0.9264217611111111}\nS=10, R=10: {'Accuracy': 0.7179, 'Precision': 0.7224119496713524, 'Recall': 0.7179, 'F1-score': 0.7170915538116818, 'AUROC': 0.9307582111111111}\nS=10, R=20: {'Accuracy': 0.7203999999999999, 'Precision': 0.7200936945977561, 'Recall': 0.7203999999999999, 'F1-score': 0.7182527588017926, 'AUROC': 0.9284757388888889}\nS=20, R=5: {'Accuracy': 0.6358999999999999, 'Precision': 0.6421541733377121, 'Recall': 0.6358999999999999, 'F1-score': 0.6295461685634913, 'AUROC': 0.8944996611111111}\nS=20, R=10: {'Accuracy': 0.6374, 'Precision': 0.6422450973202622, 'Recall': 0.6374, 'F1-score': 0.6367318282666525, 'AUROC': 0.8919029444444444}\nS=20, R=20: {'Accuracy': 0.6387, 'Precision': 0.6573699977056655, 'Recall': 0.6387, 'F1-score': 0.6418221295955533, 'AUROC': 0.8962067777777777}\nS=5, R=5, Unlearned: {'Accuracy': 0.6966145833333334, 'Precision': 0.6973464229547118, 'Recall': 0.6966145833333334, 'F1-score': 0.6954931490848295, 'AUROC': 0.9459539915360957}\nS=5, R=10, Unlearned: {'Accuracy': 0.692207532051282, 'Precision': 0.6976695247165811, 'Recall': 0.692207532051282, 'F1-score': 0.6933925806801577, 'AUROC': 0.9410048558991873}\nS=5, R=20, Unlearned: {'Accuracy': 0.7066306089743589, 'Precision': 0.710684103439594, 'Recall': 0.7066306089743589, 'F1-score': 0.7075153671800225, 'AUROC': 0.9441606840323477}\nS=10, R=5, Unlearned: {'Accuracy': 0.6496394230769231, 'Precision': 0.6505126588421758, 'Recall': 0.6496394230769231, 'F1-score': 0.6454710415216776, 'AUROC': 0.9272408355525704}\nS=10, R=10, Unlearned: {'Accuracy': 0.6405248397435898, 'Precision': 0.6465671365900002, 'Recall': 0.6405248397435898, 'F1-score': 0.6388151370489678, 'AUROC': 0.9213854498426849}\nS=10, R=20, Unlearned: {'Accuracy': 0.6595552884615384, 'Precision': 0.657778895289031, 'Recall': 0.6595552884615384, 'F1-score': 0.6540118970342513, 'AUROC': 0.927136274508378}\nS=20, R=5, Unlearned: {'Accuracy': 0.5756209935897436, 'Precision': 0.5940871920156265, 'Recall': 0.5756209935897436, 'F1-score': 0.5771477837367961, 'AUROC': 0.8957758540096867}\nS=20, R=10, Unlearned: {'Accuracy': 0.5608974358974359, 'Precision': 0.5760424377223671, 'Recall': 0.5608974358974359, 'F1-score': 0.5650026404980889, 'AUROC': 0.8896903855842706}\nS=20, R=20, Unlearned: {'Accuracy': 0.5722155448717948, 'Precision': 0.5772523272114505, 'Recall': 0.5722155448717948, 'F1-score': 0.5694218739679466, 'AUROC': 0.8916864532385305}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def calculate_losses(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return np.array(losses)\n",
        "\n",
        "def membership_inference_attack(losses_train, losses_test):\n",
        "    X = np.concatenate([losses_train, losses_test])\n",
        "    y = np.concatenate([np.ones(len(losses_train)), np.zeros(len(losses_test))])\n",
        "\n",
        "    clf = LogisticRegression(random_state=0, max_iter=1000)\n",
        "    scores = cross_val_score(clf, X.reshape(-1, 1), y, cv=5, scoring='accuracy')\n",
        "    return scores.mean()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "forget_loader = DataLoader(forget_set, batch_size=32, shuffle=False, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, drop_last=True)\n",
        "trained_models = trained_modelss['S=5, R=5']\n",
        "unlearned_models = unlearned_models_dict['S=5, R=5']\n",
        "\n",
        "forget_losses_trained = calculate_losses(trained_models[0], forget_loader, criterion)\n",
        "test_losses_trained = calculate_losses(trained_models[0], test_loader, criterion)\n",
        "forget_losses_unlearned = calculate_losses(unlearned_models[0], forget_loader, criterion)\n",
        "test_losses_unlearned = calculate_losses(unlearned_models[0], test_loader, criterion)\n",
        "\n",
        "score_trained = membership_inference_attack(forget_losses_trained, test_losses_trained)\n",
        "print(f'Membership Inference Attack Score for Trained Model: {score_trained }')\n",
        "score_unlearned = membership_inference_attack(forget_losses_unlearned, test_losses_unlearned)\n",
        "print(f'Membership Inference Attack Score for Unlearned Model: {score_unlearned}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "id": "-nfMJyGOoyMd",
        "outputId": "611c276a-c033-40b4-e53d-cae4df089ce5",
        "execution": {
          "iopub.status.busy": "2024-07-02T23:38:24.849431Z",
          "iopub.execute_input": "2024-07-02T23:38:24.849808Z",
          "iopub.status.idle": "2024-07-02T23:38:32.780939Z",
          "shell.execute_reply.started": "2024-07-02T23:38:24.849781Z",
          "shell.execute_reply": "2024-07-02T23:38:32.779843Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Membership Inference Attack Score for Trained Model: 0.9663869463869463\nMembership Inference Attack Score for Unlearned Model: 0.5163869463869464\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Custom dataset class to allow modification\n",
        "class CustomCIFAR10(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, label = self.data[index]\n",
        "        if isinstance(img, torch.Tensor):\n",
        "            img = transforms.ToPILImage()(img)  # Convert tensor to PIL image\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "# Function to add a 3x3 black square to a given image\n",
        "def add_backdoor_trigger(image, block_size=3):\n",
        "    if isinstance(image, torch.Tensor):\n",
        "        img = image.clone()\n",
        "    else:\n",
        "        img = transforms.ToTensor()(image)\n",
        "    _, h, w = img.shape\n",
        "    top_left_x = random.randint(0, w - block_size)\n",
        "    top_left_y = random.randint(0, h - block_size)\n",
        "    img[:, top_left_y:top_left_y + block_size, top_left_x:top_left_x + block_size] = 0\n",
        "    return img\n",
        "\n",
        "# Step 1: Create Poisoned Dataset\n",
        "def create_poisoned_dataset(train_dataset, target_class=0, num_poisoned=500):\n",
        "    poisoned_dataset = []\n",
        "    poisoned_indices = np.random.choice(\n",
        "        [i for i in range(len(train_dataset)) if train_dataset[i][1] == target_class],\n",
        "        num_poisoned, replace=False\n",
        "    )\n",
        "    for i in range(len(train_dataset)):\n",
        "        img, label = train_dataset[i]\n",
        "        if i in poisoned_indices:\n",
        "            img = add_backdoor_trigger(img)\n",
        "        poisoned_dataset.append((img, label))\n",
        "    return poisoned_dataset, poisoned_indices\n",
        "\n",
        "# Step 2: Train the model with the poisoned dataset\n",
        "def train_poisoned_model(poisoned_dataset):\n",
        "    shard_size = len(poisoned_dataset) // S\n",
        "    shard_indices = [indices[i:i + shard_size] for i in range(0, len(poisoned_dataset), shard_size)]\n",
        "    shard_models = []\n",
        "\n",
        "    for k in range(S):\n",
        "        shard_data = [poisoned_dataset[i] for i in shard_indices[k]]\n",
        "        shard_length = len(shard_data)\n",
        "        val_length = shard_length // 5\n",
        "        train_length = shard_length - val_length\n",
        "        train_data, val_data = random_split(shard_data, [train_length, val_length])\n",
        "        train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "        val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
        "        dataloaders = {'train': train_loader, 'val': val_loader}\n",
        "        model = CustomResNet(num_classes=10)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "        scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "        trained_model = train_model(model, dataloaders=dataloaders, criterion=criterion, optimizer=optimizer, scheduler=scheduler, num_epochs=15)\n",
        "        shard_models.append(trained_model)\n",
        "    return shard_models\n",
        "\n",
        "# Step 3: Evaluate Attack Success Rate (ASR)\n",
        "def calculate_asr(model, target_class):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    poisoned_test_set = CustomCIFAR10([(add_backdoor_trigger(test_dataset[i][0]), target_class) for i in range(len(test_dataset))], transform=transforms.ToTensor())\n",
        "    poisoned_test_loader = DataLoader(poisoned_test_set, batch_size=32, shuffle=False)\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in poisoned_test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    asr = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "    return asr\n",
        "\n",
        "# Main process\n",
        "S = 5\n",
        "R = 5\n",
        "target_class = 0\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "indices = np.arange(len(train_dataset))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# Create the poisoned dataset\n",
        "poisoned_dataset, poisoned_indices = create_poisoned_dataset(train_dataset, target_class=target_class, num_poisoned=500)\n",
        "poisoned_dataset = CustomCIFAR10(poisoned_dataset, transform=transform_train)\n",
        "\n",
        "# Train the model with the poisoned dataset\n",
        "poisoned_shard_models = train_poisoned_model(poisoned_dataset)\n",
        "\n",
        "# Evaluate the model on clean test data\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "accuracy, precision, recall, f1, auroc = evaluate_model(poisoned_shard_models[0], test_loader)  # Use one of the models for evaluation metrics\n",
        "print(f'Clean Test Data - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUROC: {auroc}')\n",
        "\n",
        "# Calculate ASR (Attack Success Rate)\n",
        "asr = calculate_asr(poisoned_shard_models[0], target_class)\n",
        "print(f'Attack Success Rate (ASR): {asr}')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T23:17:23.861170Z",
          "iopub.execute_input": "2024-07-02T23:17:23.861532Z",
          "iopub.status.idle": "2024-07-02T23:23:26.176436Z",
          "shell.execute_reply.started": "2024-07-02T23:17:23.861500Z",
          "shell.execute_reply": "2024-07-02T23:23:26.175531Z"
        },
        "trusted": true,
        "id": "y82gPSa-0ucW",
        "outputId": "61c96372-8356-4a75-cc87-4a96b4d3f427"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Files already downloaded and verified\nFiles already downloaded and verified\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Clean Test Data - Accuracy: 0.4304, Precision: 0.4920665034621721, Recall: 0.4304, F1-score: 0.42865627995426403, AUROC: 0.8304232833333334\nAttack Success Rate (ASR): 0.2445\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices = np.arange(len(train_dataset))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# Create the poisoned dataset\n",
        "poisoned_dataset, poisoned_indices = create_poisoned_dataset(train_dataset, target_class=target_class, num_poisoned=500)\n",
        "poisoned_dataset = CustomCIFAR10(poisoned_dataset, transform=transform_train)\n",
        "\n",
        "# Train the model with the poisoned dataset\n",
        "poisoned_shard_models = train_poisoned_model(poisoned_dataset)\n",
        "\n",
        "# Evaluate the model on clean test data\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "accuracy, precision, recall, f1, auroc = evaluate_model(poisoned_shard_models[0], test_loader)  # Use one of the models for evaluation metrics\n",
        "print(f'Clean Test Data - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUROC: {auroc}')\n",
        "\n",
        "# Calculate ASR (Attack Success Rate)\n",
        "asr = calculate_asr(poisoned_shard_models[0], target_class)\n",
        "print(f'Attack Success Rate (ASR): {asr}')\n",
        "\n",
        "# Unlearn the same 500 data points\n",
        "forget_indices = np.random.choice(len(poisoned_dataset), 500, replace=False)\n",
        "forget_set = [poisoned_dataset[i] for i in forget_indices]\n",
        "\n",
        "for key, shard_models in trained_modelss.items():\n",
        "    if key != 'S=5, R=5':\n",
        "        continue\n",
        "\n",
        "    S = int(key.split(\",\")[0][2:])\n",
        "    R = int(key.split(\",\")[1][3:])\n",
        "    shard_size = len(poisoned_dataset) // S\n",
        "    shard_indices = [indices[i:i + shard_size] for i in range(0, len(poisoned_dataset), shard_size)]\n",
        "\n",
        "    for k in range(S):\n",
        "        shard_data_indices = set(shard_indices[k])\n",
        "        forget_data_indices_in_shard = [idx for idx in forget_indices if idx in shard_data_indices]\n",
        "\n",
        "        if not forget_data_indices_in_shard:\n",
        "            continue\n",
        "\n",
        "        updated_shard_data = [poisoned_dataset[i] for i in shard_indices[k] if i not in forget_data_indices_in_shard]\n",
        "        shard_length = len(updated_shard_data)\n",
        "        val_length = shard_length // 5\n",
        "        train_length = shard_length - val_length\n",
        "        train_data, val_data = random_split(updated_shard_data, [train_length, val_length])\n",
        "        train_loader = DataLoader(train_data, batch_size=64, shuffle=True, drop_last=True)\n",
        "        val_loader = DataLoader(val_data, batch_size=64, shuffle=False, drop_last=True)\n",
        "        dataloaders = {'train': train_loader, 'val': val_loader}\n",
        "        model = CustomResNet(num_classes=10)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "        scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "        trained_model = train_model(model, dataloaders=dataloaders, criterion=criterion, optimizer=optimizer, scheduler=scheduler, num_epochs=15)\n",
        "        shard_models[k] = trained_model\n",
        "\n",
        "    unlearned_shard_models = shard_models\n",
        "\n",
        "# Evaluate the unlearned model on clean test data\n",
        "accuracy, precision, recall, f1, auroc = evaluate_model(unlearned_shard_models[0], test_loader)  # Use one of the models for evaluation metrics\n",
        "print(f'Unlearned Model - Clean Test Data - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUROC: {auroc}')\n",
        "\n",
        "# Calculate ASR for the unlearned model\n",
        "asr = calculate_asr(unlearned_shard_models[0], target_class)\n",
        "print(f'Unlearned Model - Attack Success Rate (ASR): {asr}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T23:27:06.685304Z",
          "iopub.execute_input": "2024-07-02T23:27:06.685992Z",
          "iopub.status.idle": "2024-07-02T23:37:27.086138Z",
          "shell.execute_reply.started": "2024-07-02T23:27:06.685959Z",
          "shell.execute_reply": "2024-07-02T23:37:27.085261Z"
        },
        "trusted": true,
        "id": "rOEwNbf-0uca",
        "outputId": "0abe6fba-af28-4f1c-cc1e-a45b0985c0a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Clean Test Data - Accuracy: 0.456, Precision: 0.5125403743450343, Recall: 0.456, F1-score: 0.463063950922401, AUROC: 0.8442392277777779\nAttack Success Rate (ASR): 0.0573\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Unlearned Model - Clean Test Data - Accuracy: 0.4863, Precision: 0.5078352667599466, Recall: 0.4863, F1-score: 0.4890600705585422, AUROC: 0.8622941777777778\nUnlearned Model - Attack Success Rate (ASR): 0.2081\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}